"""
AI Benchmarking Tool â€” Real-time metrics for all OpenRouter models.

Features:
- Benchmark any AI model via OpenRouter
- Track latency, cost, quality across models
- Thought-chain caching and reuse (Kimi-style)
- Cost tracking per user
- Model comparison dashboards
"""
import os
import time
import uuid
import hashlib
from typing import Optional, List, Dict
from datetime import datetime, timezone

from fastapi import APIRouter, Depends, HTTPException, Query
from pydantic import BaseModel

router = APIRouter(prefix="/api/benchmark", tags=["AI Benchmarking"])


# --- Models ---

class BenchmarkRequest(BaseModel):
    prompt: str
    models: List[str] = ["openai/gpt-4o-mini", "anthropic/claude-3.5-sonnet", "google/gemini-pro"]
    max_tokens: int = 500
    temperature: float = 0.7
    task_type: str = "general"  # general, coding, analysis, creative, skin_analysis


class ThoughtChainCreate(BaseModel):
    task_type: str
    domain: str = ""
    keywords: List[str] = []
    reasoning_steps: List[Dict] = []
    template: str = ""
    original_prompt: str = ""


class ThoughtChainSearch(BaseModel):
    task_type: str = ""
    domain: str = ""
    keywords: List[str] = []


# --- Endpoints ---

@router.post("/run")
async def run_benchmark(data: BenchmarkRequest):
    """Benchmark a prompt across multiple AI models simultaneously."""
    from backend.integrations.openrouter_client import OpenRouterClient
    client = OpenRouterClient()

    results = []
    for model in data.models:
        start = time.monotonic()
        try:
            response = await client.chat(
                messages=[{"role": "user", "content": data.prompt}],
                model=model,
                max_tokens=data.max_tokens,
                temperature=data.temperature,
            )
            elapsed_ms = int((time.monotonic() - start) * 1000)

            usage = response.get("usage", {})
            content = ""
            if response.get("choices"):
                content = response["choices"][0].get("message", {}).get("content", "")

            results.append({
                "model": model,
                "success": True,
                "latency_ms": elapsed_ms,
                "tokens_input": usage.get("prompt_tokens", 0),
                "tokens_output": usage.get("completion_tokens", 0),
                "total_tokens": usage.get("total_tokens", 0),
                "response_preview": content[:500],
                "response_length": len(content),
            })
        except Exception as e:
            elapsed_ms = int((time.monotonic() - start) * 1000)
            results.append({
                "model": model,
                "success": False,
                "latency_ms": elapsed_ms,
                "error": str(e),
            })

    # Sort by latency
    results.sort(key=lambda r: r.get("latency_ms", 999999))

    return {
        "benchmark_id": str(uuid.uuid4()),
        "prompt": data.prompt[:200],
        "task_type": data.task_type,
        "results": results,
        "fastest": results[0]["model"] if results and results[0].get("success") else None,
        "timestamp": datetime.now(timezone.utc).isoformat(),
    }


@router.get("/models")
async def list_available_models():
    """List all available models from OpenRouter with pricing."""
    from backend.integrations.openrouter_client import OpenRouterClient
    client = OpenRouterClient()

    try:
        models = await client.get_models()
        # Filter and format
        formatted = []
        for m in models[:100]:  # Limit to 100
            formatted.append({
                "id": m.get("id", ""),
                "name": m.get("name", ""),
                "context_length": m.get("context_length", 0),
                "pricing": m.get("pricing", {}),
                "top_provider": m.get("top_provider", {}),
            })
        return {"models": formatted, "total": len(formatted)}
    except Exception as e:
        return {"models": [], "error": str(e)}


@router.get("/usage")
async def get_usage_stats():
    """Get API usage and cost statistics."""
    from backend.integrations.openrouter_client import OpenRouterClient
    client = OpenRouterClient()

    try:
        stats = await client.get_model_stats()
        return {"usage": stats}
    except Exception as e:
        return {"usage": {}, "error": str(e)}


@router.get("/history")
async def benchmark_history(
    page: int = Query(1, ge=1),
    per_page: int = Query(20, ge=1, le=100),
    task_type: Optional[str] = None,
):
    """Get benchmark history."""
    return {
        "benchmarks": [],
        "total": 0,
        "page": page,
        "per_page": per_page,
    }


# --- Thought Chain Cache ---

@router.post("/thought-chains")
async def create_thought_chain(data: ThoughtChainCreate):
    """Cache a reasoning chain for reuse (Kimi-style thought reuse)."""
    chain_id = hashlib.sha256(
        f"{data.task_type}:{data.domain}:{':'.join(data.keywords)}".encode()
    ).hexdigest()[:16]

    return {
        "chain_id": chain_id,
        "task_type": data.task_type,
        "domain": data.domain,
        "keywords": data.keywords,
        "steps_count": len(data.reasoning_steps),
        "status": "experimental",
        "created_at": datetime.now(timezone.utc).isoformat(),
    }


@router.post("/thought-chains/search")
async def search_thought_chains(data: ThoughtChainSearch):
    """Search for reusable thought chains matching a task."""
    return {
        "chains": [],
        "total": 0,
        "search_criteria": {
            "task_type": data.task_type,
            "domain": data.domain,
            "keywords": data.keywords,
        },
    }


@router.get("/thought-chains/{chain_id}")
async def get_thought_chain(chain_id: str):
    """Retrieve a specific thought chain."""
    return {"chain_id": chain_id, "found": False}


@router.post("/thought-chains/{chain_id}/reuse")
async def reuse_thought_chain(chain_id: str, prompt: str = ""):
    """Apply a cached thought chain to a new prompt."""
    return {
        "chain_id": chain_id,
        "applied": False,
        "message": "Chain not found in cache",
    }


@router.get("/leaderboard")
async def model_leaderboard(task_type: str = "general", period: str = "7d"):
    """Get model performance leaderboard."""
    return {
        "task_type": task_type,
        "period": period,
        "leaderboard": [],
        "note": "Run benchmarks to populate the leaderboard",
    }
